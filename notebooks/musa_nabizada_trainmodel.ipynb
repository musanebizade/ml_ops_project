{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMIukTF_ipcm",
        "outputId": "7ed9abd4-ca40-44d6-c986-85c9f3e7cb17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.8.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.16.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (0.14.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Downloading category_encoders-2.8.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install category_encoders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D8-GiKki4cJ",
        "outputId": "2f546287-0002-408e-b4bf-4d2d2eb3d9db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "import warnings\n",
        "import pyarrow.parquet as pq\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "from sklearn.feature_selection import SelectPercentile, f_regression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from category_encoders import CatBoostEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.stats import ttest_rel\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "metadata": {
        "id": "GKlAzPl8iuxP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet('/content/multisim_dataset.parquet')\n",
        "pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "tKsPjx3Ti0Gt",
        "outputId": "cc6b3666-0b80-4b97-b122-4c37c656c052"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      telephone_number trf age gndr  tenure age_dev  dev_man device_os_name  \\\n",
              "9626        iWBn_13aJO   J  50    M  6957.0    1263  Samsung     Android OS   \n",
              "34347       8JWaD2wYoQ   M  52    M  1041.0     125  Samsung     Android OS   \n",
              "72797       sbPnQURi3j   A  49    U  7176.0     345  Samsung     Android OS   \n",
              "68025       qOy-JlqPTl   M  49    M  4309.0     466  Samsung     Android OS   \n",
              "31781       MNODhcb7h0   A  50    U  6999.0     695  Samsung     Android OS   \n",
              "\n",
              "      dev_num is_dualsim is_featurephone is_smartphone simcard_type   region  \\\n",
              "9626        1          1               0             1         None     GAKH   \n",
              "34347       2          1               0             1           4G  MASALLI   \n",
              "72797       1          1               0             1           4G   SAATLI   \n",
              "68025       1          1               0             1           4G     BAKU   \n",
              "31781       1          1               0             1           4G   ASTARA   \n",
              "\n",
              "       val2_1  val3_1  val4_1  val5_1  val6_1  val7_1  val8_1  val9_1  \\\n",
              "9626      3.0     3.0      14     7.4       0     0.0       0     0.0   \n",
              "34347     2.0     2.0       6     2.9       0     0.0       0     0.0   \n",
              "72797     9.0     2.0     103   138.5       2     0.9       2     0.9   \n",
              "68025    25.0     1.0     133   164.3     142   187.9      58    53.7   \n",
              "31781     9.0     3.0     102    85.1       1     0.1       1     0.1   \n",
              "\n",
              "       val10_1  val11_1       val12_1       val13_1  val14_1  val15_1  \\\n",
              "9626         0      0.0      0.000000      0.000000      0.0     0.00   \n",
              "34347        0      0.0    176.447991      0.000000      0.0     0.00   \n",
              "72797        0      0.0  36194.190184  36133.004917      0.0     0.10   \n",
              "68025       79    129.0   8181.310782     49.985725      0.0     0.00   \n",
              "31781        0      0.0   1271.287395   1268.585261      0.0     0.05   \n",
              "\n",
              "       val16_1  val17_1  val18_1  val19_1  val20_1  val21_1  val2_2  val3_2  \\\n",
              "9626      1.12     0.00      0.0       79       14        0    3.00     3.0   \n",
              "34347     0.40     0.00      0.0      118        6        0    1.06     1.0   \n",
              "72797     2.80     0.10      0.0      302      103        2    5.00     1.0   \n",
              "68025     0.00     0.00      0.0      411      133      142   25.00     1.0   \n",
              "31781     0.55     0.05      0.0      169      102        1   10.00     4.0   \n",
              "\n",
              "       val4_2  val5_2  val6_2  val7_2  val8_2  val9_2  val10_2  val11_2  \\\n",
              "9626     13.0     9.0     1.0     0.4     1.0     0.4      0.0      0.0   \n",
              "34347    29.0    21.4     1.0     0.2     0.0     0.0      1.0      0.2   \n",
              "72797    80.0    86.0     4.0     5.7     1.0     4.1      3.0      1.6   \n",
              "68025   209.0   273.0   193.0   192.0    61.0    49.1    126.0    124.3   \n",
              "31781   117.0    97.6     1.0     1.0     1.0     1.0      0.0      0.0   \n",
              "\n",
              "            val12_2       val13_2  val14_2  val15_2  val16_2  val17_2  \\\n",
              "9626       0.000000      0.000000      0.0     0.14     1.05     0.14   \n",
              "34347    117.043976    116.052454      0.0     0.08     3.12     0.00   \n",
              "72797  13857.365302  13827.253128      0.0     0.50     1.70     0.25   \n",
              "68025   7804.867967     65.904087      0.0     0.00     0.00     0.00   \n",
              "31781    106.041120    105.620465      0.0     0.05     0.00     0.05   \n",
              "\n",
              "       val18_2  val19_2  val20_2  val21_2  val2_3  val3_3  val4_3  val5_3  \\\n",
              "9626      0.00     64.0     13.0      1.0    3.00     3.0    14.0    10.3   \n",
              "34347     0.08    118.0     29.0      1.0    5.71     3.0    26.0    34.4   \n",
              "72797     0.25    292.0     80.0      4.0    5.00     1.0    74.0   129.5   \n",
              "68025     0.00    622.0    209.0    193.0   25.00     1.0   186.0   144.2   \n",
              "31781     0.00    185.0    117.0      1.0   12.00     4.0   117.0   104.6   \n",
              "\n",
              "       val6_3  val7_3  val8_3  val9_3  val10_3  val11_3       val12_3  \\\n",
              "9626      0.0     0.0     0.0     0.0      0.0      0.0      0.000000   \n",
              "34347     5.0     1.8     2.0     0.8      2.0      0.8    160.147566   \n",
              "72797     2.0     2.1     1.0     0.4      1.0      1.8      0.000000   \n",
              "68025   172.0   192.0    44.0    46.8    116.0    129.1  11567.962663   \n",
              "31781     2.0     0.4     0.0     0.0      0.0      0.0      0.000000   \n",
              "\n",
              "         val13_3  val14_3  val15_3  val16_3  val17_3  val18_3  val19_3  \\\n",
              "9626    0.000000      0.0     0.00     0.42     0.00     0.00     68.0   \n",
              "34347  67.178972      0.0     0.40     1.28     0.16     0.16    125.0   \n",
              "72797   0.000000      0.0     0.15     1.70     0.05     0.10    238.0   \n",
              "68025  66.429559      0.0     0.00     0.00     0.00     0.00    542.0   \n",
              "31781   0.000000      0.0     0.10     0.35     0.00     0.00    160.0   \n",
              "\n",
              "       val20_3  val21_3  val2_4  val4_4  val5_4  val6_4  val7_4  val8_4  \\\n",
              "9626      14.0      0.0     2.0     4.0     2.8     0.0     0.0     0.0   \n",
              "34347     26.0      5.0     1.0     1.0     3.2     0.0     0.0     0.0   \n",
              "72797     74.0      2.0     3.0    62.0   121.0     7.0     6.7     1.0   \n",
              "68025    186.0    172.0    25.0   115.0   130.7    97.0   124.5    29.0   \n",
              "31781    117.0      2.0     8.0   100.0   111.7     2.0    20.4     1.0   \n",
              "\n",
              "       val9_4  val10_4  val11_4      val12_4     val13_4  val14_4  val15_4  \\\n",
              "9626      0.0      0.0      0.0     0.000000    0.000000      0.0     0.00   \n",
              "34347     0.0      0.0      0.0   182.922203  181.783154      0.0     0.00   \n",
              "72797     0.6      6.0      6.1     0.000000    0.000000      0.0     0.65   \n",
              "68025    34.6     68.0     89.8  8293.154886   44.728278      0.0     0.00   \n",
              "31781     1.0      1.0     19.4     0.000000    0.000000      0.0     1.05   \n",
              "\n",
              "            val16_4  val17_4  val18_4  val19_4  val20_4  val21_4  val2_5  \\\n",
              "9626  -1.490116e-09     0.00      0.0     39.0      4.0      0.0     3.0   \n",
              "34347  0.000000e+00     0.00      0.0     81.0      1.0      0.0     3.0   \n",
              "72797  2.500000e+00     0.05      0.6    239.0     62.0      7.0     5.0   \n",
              "68025  0.000000e+00     0.00      0.0    330.0    115.0     97.0    25.0   \n",
              "31781  1.500000e-01     0.05      1.0    150.0    100.0      2.0    10.0   \n",
              "\n",
              "       val4_5  val5_5  val6_5  val7_5  val8_5  val9_5  val10_5  val11_5  \\\n",
              "9626     10.0     8.3     0.0     0.0     0.0     0.0      0.0      0.0   \n",
              "34347     7.0     4.4     0.0     0.0     0.0     0.0      0.0      0.0   \n",
              "72797    80.0   202.6     3.0     3.9     0.0     0.0      3.0      3.9   \n",
              "68025   100.0    97.5   114.0   103.0    42.0    35.4     72.0     67.6   \n",
              "31781   114.0   227.3     1.0     1.5     1.0     1.5      0.0      0.0   \n",
              "\n",
              "            val12_5     val13_5  val14_5  val15_5  val16_5  val17_5  val18_5  \\\n",
              "9626       0.000000    0.000000      0.0      0.0     0.91      0.0      0.0   \n",
              "34347    463.484166  198.153160      0.0      0.0     0.64      0.0      0.0   \n",
              "72797      0.000000    0.000000      0.0      0.5     2.35      0.0      0.5   \n",
              "68025  10192.908145   65.786654      0.0      0.0     0.00      0.0      0.0   \n",
              "31781      0.000000    0.000000      0.0      0.1     0.00      0.1      0.0   \n",
              "\n",
              "       val19_5  val20_5  val21_5  val2_6  val4_6  val5_6  val6_6  val7_6  \\\n",
              "9626      57.0     10.0      0.0     3.0     7.0     4.4     0.0     0.0   \n",
              "34347     88.0      7.0      0.0     2.0     0.0     0.0     2.0     4.3   \n",
              "72797    219.0     80.0      3.0     5.0    87.0   110.6     4.0     2.7   \n",
              "68025    307.0    100.0    114.0    25.0   142.0   121.3    80.0    60.6   \n",
              "31781    132.0    114.0      1.0    10.0    96.0   163.1     0.0     0.0   \n",
              "\n",
              "       val8_6  val9_6  val10_6  val11_6      val12_6     val13_6  val14_6  \\\n",
              "9626      0.0     0.0      0.0      0.0     0.000000    0.000000      0.0   \n",
              "34347     1.0     1.4      1.0      3.0   207.238037  206.536585      0.0   \n",
              "72797     0.0     0.0      4.0      2.7     0.000000    0.000000      0.0   \n",
              "68025    29.0    21.0     51.0     39.6  9260.032768   51.885746      0.0   \n",
              "31781     0.0     0.0      0.0      0.0     0.000000    0.000000      0.0   \n",
              "\n",
              "       val15_6  val16_6  val17_6  val18_6  val19_6  val20_6  val21_6  target  \n",
              "9626      0.00     0.56     0.00     0.00     60.0      7.0      0.0       1  \n",
              "34347     0.40     0.00     0.16     0.24    156.0      0.0      2.0       1  \n",
              "72797     0.25     2.35     0.00     0.25    241.0     87.0      4.0       1  \n",
              "68025     0.00     0.00     0.00     0.00    299.0    142.0     80.0       1  \n",
              "31781     0.00     0.80     0.00     0.00    121.0     96.0      0.0       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e9ba4e21-3e54-45d8-b52a-3083927aba2a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>telephone_number</th>\n",
              "      <th>trf</th>\n",
              "      <th>age</th>\n",
              "      <th>gndr</th>\n",
              "      <th>tenure</th>\n",
              "      <th>age_dev</th>\n",
              "      <th>dev_man</th>\n",
              "      <th>device_os_name</th>\n",
              "      <th>dev_num</th>\n",
              "      <th>is_dualsim</th>\n",
              "      <th>is_featurephone</th>\n",
              "      <th>is_smartphone</th>\n",
              "      <th>simcard_type</th>\n",
              "      <th>region</th>\n",
              "      <th>val2_1</th>\n",
              "      <th>val3_1</th>\n",
              "      <th>val4_1</th>\n",
              "      <th>val5_1</th>\n",
              "      <th>val6_1</th>\n",
              "      <th>val7_1</th>\n",
              "      <th>val8_1</th>\n",
              "      <th>val9_1</th>\n",
              "      <th>val10_1</th>\n",
              "      <th>val11_1</th>\n",
              "      <th>val12_1</th>\n",
              "      <th>val13_1</th>\n",
              "      <th>val14_1</th>\n",
              "      <th>val15_1</th>\n",
              "      <th>val16_1</th>\n",
              "      <th>val17_1</th>\n",
              "      <th>val18_1</th>\n",
              "      <th>val19_1</th>\n",
              "      <th>val20_1</th>\n",
              "      <th>val21_1</th>\n",
              "      <th>val2_2</th>\n",
              "      <th>val3_2</th>\n",
              "      <th>val4_2</th>\n",
              "      <th>val5_2</th>\n",
              "      <th>val6_2</th>\n",
              "      <th>val7_2</th>\n",
              "      <th>val8_2</th>\n",
              "      <th>val9_2</th>\n",
              "      <th>val10_2</th>\n",
              "      <th>val11_2</th>\n",
              "      <th>val12_2</th>\n",
              "      <th>val13_2</th>\n",
              "      <th>val14_2</th>\n",
              "      <th>val15_2</th>\n",
              "      <th>val16_2</th>\n",
              "      <th>val17_2</th>\n",
              "      <th>val18_2</th>\n",
              "      <th>val19_2</th>\n",
              "      <th>val20_2</th>\n",
              "      <th>val21_2</th>\n",
              "      <th>val2_3</th>\n",
              "      <th>val3_3</th>\n",
              "      <th>val4_3</th>\n",
              "      <th>val5_3</th>\n",
              "      <th>val6_3</th>\n",
              "      <th>val7_3</th>\n",
              "      <th>val8_3</th>\n",
              "      <th>val9_3</th>\n",
              "      <th>val10_3</th>\n",
              "      <th>val11_3</th>\n",
              "      <th>val12_3</th>\n",
              "      <th>val13_3</th>\n",
              "      <th>val14_3</th>\n",
              "      <th>val15_3</th>\n",
              "      <th>val16_3</th>\n",
              "      <th>val17_3</th>\n",
              "      <th>val18_3</th>\n",
              "      <th>val19_3</th>\n",
              "      <th>val20_3</th>\n",
              "      <th>val21_3</th>\n",
              "      <th>val2_4</th>\n",
              "      <th>val4_4</th>\n",
              "      <th>val5_4</th>\n",
              "      <th>val6_4</th>\n",
              "      <th>val7_4</th>\n",
              "      <th>val8_4</th>\n",
              "      <th>val9_4</th>\n",
              "      <th>val10_4</th>\n",
              "      <th>val11_4</th>\n",
              "      <th>val12_4</th>\n",
              "      <th>val13_4</th>\n",
              "      <th>val14_4</th>\n",
              "      <th>val15_4</th>\n",
              "      <th>val16_4</th>\n",
              "      <th>val17_4</th>\n",
              "      <th>val18_4</th>\n",
              "      <th>val19_4</th>\n",
              "      <th>val20_4</th>\n",
              "      <th>val21_4</th>\n",
              "      <th>val2_5</th>\n",
              "      <th>val4_5</th>\n",
              "      <th>val5_5</th>\n",
              "      <th>val6_5</th>\n",
              "      <th>val7_5</th>\n",
              "      <th>val8_5</th>\n",
              "      <th>val9_5</th>\n",
              "      <th>val10_5</th>\n",
              "      <th>val11_5</th>\n",
              "      <th>val12_5</th>\n",
              "      <th>val13_5</th>\n",
              "      <th>val14_5</th>\n",
              "      <th>val15_5</th>\n",
              "      <th>val16_5</th>\n",
              "      <th>val17_5</th>\n",
              "      <th>val18_5</th>\n",
              "      <th>val19_5</th>\n",
              "      <th>val20_5</th>\n",
              "      <th>val21_5</th>\n",
              "      <th>val2_6</th>\n",
              "      <th>val4_6</th>\n",
              "      <th>val5_6</th>\n",
              "      <th>val6_6</th>\n",
              "      <th>val7_6</th>\n",
              "      <th>val8_6</th>\n",
              "      <th>val9_6</th>\n",
              "      <th>val10_6</th>\n",
              "      <th>val11_6</th>\n",
              "      <th>val12_6</th>\n",
              "      <th>val13_6</th>\n",
              "      <th>val14_6</th>\n",
              "      <th>val15_6</th>\n",
              "      <th>val16_6</th>\n",
              "      <th>val17_6</th>\n",
              "      <th>val18_6</th>\n",
              "      <th>val19_6</th>\n",
              "      <th>val20_6</th>\n",
              "      <th>val21_6</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9626</th>\n",
              "      <td>iWBn_13aJO</td>\n",
              "      <td>J</td>\n",
              "      <td>50</td>\n",
              "      <td>M</td>\n",
              "      <td>6957.0</td>\n",
              "      <td>1263</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>Android OS</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>GAKH</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>14</td>\n",
              "      <td>7.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>79</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>1.05</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.00</td>\n",
              "      <td>64.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>10.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>68.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-1.490116e-09</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>60.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34347</th>\n",
              "      <td>8JWaD2wYoQ</td>\n",
              "      <td>M</td>\n",
              "      <td>52</td>\n",
              "      <td>M</td>\n",
              "      <td>1041.0</td>\n",
              "      <td>125</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>Android OS</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4G</td>\n",
              "      <td>MASALLI</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6</td>\n",
              "      <td>2.9</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>176.447991</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>118</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1.06</td>\n",
              "      <td>1.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>21.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>117.043976</td>\n",
              "      <td>116.052454</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.08</td>\n",
              "      <td>3.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.08</td>\n",
              "      <td>118.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.71</td>\n",
              "      <td>3.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>34.4</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>160.147566</td>\n",
              "      <td>67.178972</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.40</td>\n",
              "      <td>1.28</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.16</td>\n",
              "      <td>125.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>182.922203</td>\n",
              "      <td>181.783154</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>463.484166</td>\n",
              "      <td>198.153160</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>207.238037</td>\n",
              "      <td>206.536585</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.24</td>\n",
              "      <td>156.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72797</th>\n",
              "      <td>sbPnQURi3j</td>\n",
              "      <td>A</td>\n",
              "      <td>49</td>\n",
              "      <td>U</td>\n",
              "      <td>7176.0</td>\n",
              "      <td>345</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>Android OS</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4G</td>\n",
              "      <td>SAATLI</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>103</td>\n",
              "      <td>138.5</td>\n",
              "      <td>2</td>\n",
              "      <td>0.9</td>\n",
              "      <td>2</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36194.190184</td>\n",
              "      <td>36133.004917</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2.80</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>302</td>\n",
              "      <td>103</td>\n",
              "      <td>2</td>\n",
              "      <td>5.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>13857.365302</td>\n",
              "      <td>13827.253128</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.70</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>292.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>129.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.15</td>\n",
              "      <td>1.70</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.10</td>\n",
              "      <td>238.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.65</td>\n",
              "      <td>2.500000e+00</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.6</td>\n",
              "      <td>239.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>202.6</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.9</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.35</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>219.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>110.6</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.7</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>2.35</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>241.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68025</th>\n",
              "      <td>qOy-JlqPTl</td>\n",
              "      <td>M</td>\n",
              "      <td>49</td>\n",
              "      <td>M</td>\n",
              "      <td>4309.0</td>\n",
              "      <td>466</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>Android OS</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4G</td>\n",
              "      <td>BAKU</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>133</td>\n",
              "      <td>164.3</td>\n",
              "      <td>142</td>\n",
              "      <td>187.9</td>\n",
              "      <td>58</td>\n",
              "      <td>53.7</td>\n",
              "      <td>79</td>\n",
              "      <td>129.0</td>\n",
              "      <td>8181.310782</td>\n",
              "      <td>49.985725</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>411</td>\n",
              "      <td>133</td>\n",
              "      <td>142</td>\n",
              "      <td>25.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>209.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>49.1</td>\n",
              "      <td>126.0</td>\n",
              "      <td>124.3</td>\n",
              "      <td>7804.867967</td>\n",
              "      <td>65.904087</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>622.0</td>\n",
              "      <td>209.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>25.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>144.2</td>\n",
              "      <td>172.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>46.8</td>\n",
              "      <td>116.0</td>\n",
              "      <td>129.1</td>\n",
              "      <td>11567.962663</td>\n",
              "      <td>66.429559</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>542.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>130.7</td>\n",
              "      <td>97.0</td>\n",
              "      <td>124.5</td>\n",
              "      <td>29.0</td>\n",
              "      <td>34.6</td>\n",
              "      <td>68.0</td>\n",
              "      <td>89.8</td>\n",
              "      <td>8293.154886</td>\n",
              "      <td>44.728278</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>330.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>97.5</td>\n",
              "      <td>114.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>35.4</td>\n",
              "      <td>72.0</td>\n",
              "      <td>67.6</td>\n",
              "      <td>10192.908145</td>\n",
              "      <td>65.786654</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>307.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>121.3</td>\n",
              "      <td>80.0</td>\n",
              "      <td>60.6</td>\n",
              "      <td>29.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>39.6</td>\n",
              "      <td>9260.032768</td>\n",
              "      <td>51.885746</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>299.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31781</th>\n",
              "      <td>MNODhcb7h0</td>\n",
              "      <td>A</td>\n",
              "      <td>50</td>\n",
              "      <td>U</td>\n",
              "      <td>6999.0</td>\n",
              "      <td>695</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>Android OS</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4G</td>\n",
              "      <td>ASTARA</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>102</td>\n",
              "      <td>85.1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1271.287395</td>\n",
              "      <td>1268.585261</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>169</td>\n",
              "      <td>102</td>\n",
              "      <td>1</td>\n",
              "      <td>10.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>97.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>106.041120</td>\n",
              "      <td>105.620465</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>185.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>12.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>104.6</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>160.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>111.7</td>\n",
              "      <td>2.0</td>\n",
              "      <td>20.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.05</td>\n",
              "      <td>1.500000e-01</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>227.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>163.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>121.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9ba4e21-3e54-45d8-b52a-3083927aba2a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e9ba4e21-3e54-45d8-b52a-3083927aba2a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e9ba4e21-3e54-45d8-b52a-3083927aba2a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-84f9adc3-13dc-4374-8e6a-7c464187a108\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-84f9adc3-13dc-4374-8e6a-7c464187a108')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-84f9adc3-13dc-4374-8e6a-7c464187a108 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-g437fvjB0G",
        "outputId": "fa51603d-3d91-476f-d93e-7522a78b3032"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 10000 entries, 9626 to 90423\n",
            "Columns: 132 entries, telephone_number to target\n",
            "dtypes: float64(111), int64(8), object(13)\n",
            "memory usage: 10.1+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfj1U1ASjFg-",
        "outputId": "810610c7-8d26-4add-9c1e-97a8ae946d8d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['telephone_number', 'trf', 'age', 'gndr', 'tenure', 'age_dev', 'dev_man', 'device_os_name', 'dev_num', 'is_dualsim', 'is_featurephone', 'is_smartphone', 'simcard_type', 'region', 'val2_1', 'val3_1', 'val4_1', 'val5_1', 'val6_1', 'val7_1', 'val8_1', 'val9_1', 'val10_1', 'val11_1', 'val12_1', 'val13_1', 'val14_1', 'val15_1', 'val16_1', 'val17_1', 'val18_1', 'val19_1', 'val20_1', 'val21_1', 'val2_2', 'val3_2', 'val4_2', 'val5_2', 'val6_2', 'val7_2', 'val8_2', 'val9_2', 'val10_2', 'val11_2', 'val12_2', 'val13_2', 'val14_2', 'val15_2', 'val16_2', 'val17_2', 'val18_2', 'val19_2', 'val20_2', 'val21_2', 'val2_3', 'val3_3', 'val4_3', 'val5_3', 'val6_3', 'val7_3', 'val8_3', 'val9_3', 'val10_3', 'val11_3', 'val12_3', 'val13_3', 'val14_3', 'val15_3', 'val16_3', 'val17_3', 'val18_3', 'val19_3', 'val20_3', 'val21_3', 'val2_4', 'val4_4', 'val5_4', 'val6_4', 'val7_4', 'val8_4', 'val9_4', 'val10_4', 'val11_4', 'val12_4', 'val13_4', 'val14_4', 'val15_4', 'val16_4', 'val17_4', 'val18_4', 'val19_4', 'val20_4', 'val21_4', 'val2_5', 'val4_5', 'val5_5', 'val6_5', 'val7_5', 'val8_5', 'val9_5', 'val10_5', 'val11_5', 'val12_5', 'val13_5', 'val14_5', 'val15_5', 'val16_5', 'val17_5', 'val18_5', 'val19_5', 'val20_5', 'val21_5', 'val2_6', 'val4_6', 'val5_6', 'val6_6', 'val7_6', 'val8_6', 'val9_6', 'val10_6', 'val11_6', 'val12_6', 'val13_6', 'val14_6', 'val15_6', 'val16_6', 'val17_6', 'val18_6', 'val19_6', 'val20_6', 'val21_6', 'target']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only object columns\n",
        "df_object = df.select_dtypes(include=['object'])\n",
        "\n",
        "# Show info about them\n",
        "df_object.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lA2UfIjfjT-Y",
        "outputId": "379590f4-0aac-4fde-9ef3-ad3d0215190d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 10000 entries, 9626 to 90423\n",
            "Data columns (total 13 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   telephone_number  10000 non-null  object\n",
            " 1   trf               10000 non-null  object\n",
            " 2   age               9976 non-null   object\n",
            " 3   gndr              10000 non-null  object\n",
            " 4   age_dev           9980 non-null   object\n",
            " 5   dev_man           9980 non-null   object\n",
            " 6   device_os_name    9980 non-null   object\n",
            " 7   dev_num           9980 non-null   object\n",
            " 8   is_dualsim        10000 non-null  object\n",
            " 9   is_featurephone   10000 non-null  object\n",
            " 10  is_smartphone     10000 non-null  object\n",
            " 11  simcard_type      9651 non-null   object\n",
            " 12  region            9904 non-null   object\n",
            "dtypes: object(13)\n",
            "memory usage: 1.1+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_regression\n",
        "# Select features & target\n",
        "X = df.drop(columns=['target'])\n",
        "y = df['target']\n",
        "\n",
        "# Encode object columns\n",
        "X_encoded = X.copy()\n",
        "for col in X_encoded.select_dtypes(include='object'):\n",
        "    X_encoded[col] = X_encoded[col].astype('category').cat.codes\n",
        "\n",
        "# Fill NaNs with a safe placeholder\n",
        "X_encoded = X_encoded.fillna(-999)\n",
        "y = y.fillna(y.mean())\n",
        "\n",
        "mi_scores = mutual_info_regression(X_encoded, y, random_state=42)\n",
        "mi_series = pd.Series(mi_scores, index=X_encoded.columns).sort_values(ascending=False)\n",
        "print(mi_series.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qoa7YYN5jX6Y",
        "outputId": "087fff40-549a-419a-cc0e-ee54cf526340"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val20_4    0.027031\n",
            "val4_4     0.024791\n",
            "val18_2    0.023587\n",
            "val7_1     0.021939\n",
            "val20_2    0.019226\n",
            "val20_5    0.018787\n",
            "val16_4    0.017719\n",
            "val16_5    0.017012\n",
            "trf        0.016835\n",
            "val17_2    0.016686\n",
            "val15_5    0.015876\n",
            "region     0.014995\n",
            "val9_6     0.014015\n",
            "val15_4    0.013647\n",
            "val7_2     0.013169\n",
            "val14_6    0.012827\n",
            "val14_2    0.012796\n",
            "val5_2     0.012678\n",
            "val16_1    0.012611\n",
            "val15_2    0.011713\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mutual Information was applied to identify which features are most related to the target, helping us select important features and reduce noise before modeling.\n"
      ],
      "metadata": {
        "id": "Yk-k3XuSzMz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# Temporary encoded dataframe\n",
        "df_cat_encoded = df[cat_cols].apply(lambda x: x.astype('category').cat.codes)\n",
        "df_cat_encoded['target'] = df['target']"
      ],
      "metadata": {
        "id": "08dwgNBIs2rC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "X_cat = df_cat_encoded.drop(columns=['target'])\n",
        "y = df_cat_encoded['target']\n",
        "\n",
        "mi_scores = mutual_info_regression(X_cat, y, discrete_features=True, random_state=42)\n",
        "mi_series = pd.Series(mi_scores, index=X_cat.columns).sort_values(ascending=False)\n",
        "print(mi_series)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBSqRxM6s4ey",
        "outputId": "aee9fe8e-5015-4df8-c57e-90e5330c1b8e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age_dev             0.629722\n",
            "region              0.021172\n",
            "trf                 0.014260\n",
            "dev_man             0.012172\n",
            "simcard_type        0.008814\n",
            "gndr                0.008529\n",
            "device_os_name      0.005626\n",
            "dev_num             0.005594\n",
            "age                 0.003922\n",
            "is_dualsim          0.003259\n",
            "is_smartphone       0.002556\n",
            "is_featurephone     0.002556\n",
            "telephone_number    0.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " As we can see, is_smartphone and is_featurephone columns have less importance. Thats why we drop them"
      ],
      "metadata": {
        "id": "tmbApWhUtaFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.set_index('telephone_number', inplace=True)\n",
        "df = df.drop(['is_smartphone', 'is_featurephone'], axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "_P1PzLK6tyI6",
        "outputId": "7b72acfe-e23b-4512-9f62-d83a72ef9496"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 trf age gndr  tenure age_dev  dev_man device_os_name dev_num  \\\n",
              "telephone_number                                                                \n",
              "iWBn_13aJO         J  50    M  6957.0    1263  Samsung     Android OS       1   \n",
              "8JWaD2wYoQ         M  52    M  1041.0     125  Samsung     Android OS       2   \n",
              "sbPnQURi3j         A  49    U  7176.0     345  Samsung     Android OS       1   \n",
              "qOy-JlqPTl         M  49    M  4309.0     466  Samsung     Android OS       1   \n",
              "MNODhcb7h0         A  50    U  6999.0     695  Samsung     Android OS       1   \n",
              "\n",
              "                 is_dualsim simcard_type   region  val2_1  val3_1  val4_1  \\\n",
              "telephone_number                                                            \n",
              "iWBn_13aJO                1         None     GAKH     3.0     3.0      14   \n",
              "8JWaD2wYoQ                1           4G  MASALLI     2.0     2.0       6   \n",
              "sbPnQURi3j                1           4G   SAATLI     9.0     2.0     103   \n",
              "qOy-JlqPTl                1           4G     BAKU    25.0     1.0     133   \n",
              "MNODhcb7h0                1           4G   ASTARA     9.0     3.0     102   \n",
              "\n",
              "                  val5_1  val6_1  val7_1  val8_1  val9_1  val10_1  val11_1  \\\n",
              "telephone_number                                                             \n",
              "iWBn_13aJO           7.4       0     0.0       0     0.0        0      0.0   \n",
              "8JWaD2wYoQ           2.9       0     0.0       0     0.0        0      0.0   \n",
              "sbPnQURi3j         138.5       2     0.9       2     0.9        0      0.0   \n",
              "qOy-JlqPTl         164.3     142   187.9      58    53.7       79    129.0   \n",
              "MNODhcb7h0          85.1       1     0.1       1     0.1        0      0.0   \n",
              "\n",
              "                       val12_1       val13_1  val14_1  val15_1  val16_1  \\\n",
              "telephone_number                                                          \n",
              "iWBn_13aJO            0.000000      0.000000      0.0     0.00     1.12   \n",
              "8JWaD2wYoQ          176.447991      0.000000      0.0     0.00     0.40   \n",
              "sbPnQURi3j        36194.190184  36133.004917      0.0     0.10     2.80   \n",
              "qOy-JlqPTl         8181.310782     49.985725      0.0     0.00     0.00   \n",
              "MNODhcb7h0         1271.287395   1268.585261      0.0     0.05     0.55   \n",
              "\n",
              "                  val17_1  val18_1  val19_1  val20_1  val21_1  val2_2  val3_2  \\\n",
              "telephone_number                                                                \n",
              "iWBn_13aJO           0.00      0.0       79       14        0    3.00     3.0   \n",
              "8JWaD2wYoQ           0.00      0.0      118        6        0    1.06     1.0   \n",
              "sbPnQURi3j           0.10      0.0      302      103        2    5.00     1.0   \n",
              "qOy-JlqPTl           0.00      0.0      411      133      142   25.00     1.0   \n",
              "MNODhcb7h0           0.05      0.0      169      102        1   10.00     4.0   \n",
              "\n",
              "                  val4_2  val5_2  val6_2  val7_2  val8_2  val9_2  val10_2  \\\n",
              "telephone_number                                                            \n",
              "iWBn_13aJO          13.0     9.0     1.0     0.4     1.0     0.4      0.0   \n",
              "8JWaD2wYoQ          29.0    21.4     1.0     0.2     0.0     0.0      1.0   \n",
              "sbPnQURi3j          80.0    86.0     4.0     5.7     1.0     4.1      3.0   \n",
              "qOy-JlqPTl         209.0   273.0   193.0   192.0    61.0    49.1    126.0   \n",
              "MNODhcb7h0         117.0    97.6     1.0     1.0     1.0     1.0      0.0   \n",
              "\n",
              "                  val11_2       val12_2       val13_2  val14_2  val15_2  \\\n",
              "telephone_number                                                          \n",
              "iWBn_13aJO            0.0      0.000000      0.000000      0.0     0.14   \n",
              "8JWaD2wYoQ            0.2    117.043976    116.052454      0.0     0.08   \n",
              "sbPnQURi3j            1.6  13857.365302  13827.253128      0.0     0.50   \n",
              "qOy-JlqPTl          124.3   7804.867967     65.904087      0.0     0.00   \n",
              "MNODhcb7h0            0.0    106.041120    105.620465      0.0     0.05   \n",
              "\n",
              "                  val16_2  val17_2  val18_2  val19_2  val20_2  val21_2  \\\n",
              "telephone_number                                                         \n",
              "iWBn_13aJO           1.05     0.14     0.00     64.0     13.0      1.0   \n",
              "8JWaD2wYoQ           3.12     0.00     0.08    118.0     29.0      1.0   \n",
              "sbPnQURi3j           1.70     0.25     0.25    292.0     80.0      4.0   \n",
              "qOy-JlqPTl           0.00     0.00     0.00    622.0    209.0    193.0   \n",
              "MNODhcb7h0           0.00     0.05     0.00    185.0    117.0      1.0   \n",
              "\n",
              "                  val2_3  val3_3  val4_3  val5_3  val6_3  val7_3  val8_3  \\\n",
              "telephone_number                                                           \n",
              "iWBn_13aJO          3.00     3.0    14.0    10.3     0.0     0.0     0.0   \n",
              "8JWaD2wYoQ          5.71     3.0    26.0    34.4     5.0     1.8     2.0   \n",
              "sbPnQURi3j          5.00     1.0    74.0   129.5     2.0     2.1     1.0   \n",
              "qOy-JlqPTl         25.00     1.0   186.0   144.2   172.0   192.0    44.0   \n",
              "MNODhcb7h0         12.00     4.0   117.0   104.6     2.0     0.4     0.0   \n",
              "\n",
              "                  val9_3  val10_3  val11_3       val12_3    val13_3  val14_3  \\\n",
              "telephone_number                                                               \n",
              "iWBn_13aJO           0.0      0.0      0.0      0.000000   0.000000      0.0   \n",
              "8JWaD2wYoQ           0.8      2.0      0.8    160.147566  67.178972      0.0   \n",
              "sbPnQURi3j           0.4      1.0      1.8      0.000000   0.000000      0.0   \n",
              "qOy-JlqPTl          46.8    116.0    129.1  11567.962663  66.429559      0.0   \n",
              "MNODhcb7h0           0.0      0.0      0.0      0.000000   0.000000      0.0   \n",
              "\n",
              "                  val15_3  val16_3  val17_3  val18_3  val19_3  val20_3  \\\n",
              "telephone_number                                                         \n",
              "iWBn_13aJO           0.00     0.42     0.00     0.00     68.0     14.0   \n",
              "8JWaD2wYoQ           0.40     1.28     0.16     0.16    125.0     26.0   \n",
              "sbPnQURi3j           0.15     1.70     0.05     0.10    238.0     74.0   \n",
              "qOy-JlqPTl           0.00     0.00     0.00     0.00    542.0    186.0   \n",
              "MNODhcb7h0           0.10     0.35     0.00     0.00    160.0    117.0   \n",
              "\n",
              "                  val21_3  val2_4  val4_4  val5_4  val6_4  val7_4  val8_4  \\\n",
              "telephone_number                                                            \n",
              "iWBn_13aJO            0.0     2.0     4.0     2.8     0.0     0.0     0.0   \n",
              "8JWaD2wYoQ            5.0     1.0     1.0     3.2     0.0     0.0     0.0   \n",
              "sbPnQURi3j            2.0     3.0    62.0   121.0     7.0     6.7     1.0   \n",
              "qOy-JlqPTl          172.0    25.0   115.0   130.7    97.0   124.5    29.0   \n",
              "MNODhcb7h0            2.0     8.0   100.0   111.7     2.0    20.4     1.0   \n",
              "\n",
              "                  val9_4  val10_4  val11_4      val12_4     val13_4  val14_4  \\\n",
              "telephone_number                                                               \n",
              "iWBn_13aJO           0.0      0.0      0.0     0.000000    0.000000      0.0   \n",
              "8JWaD2wYoQ           0.0      0.0      0.0   182.922203  181.783154      0.0   \n",
              "sbPnQURi3j           0.6      6.0      6.1     0.000000    0.000000      0.0   \n",
              "qOy-JlqPTl          34.6     68.0     89.8  8293.154886   44.728278      0.0   \n",
              "MNODhcb7h0           1.0      1.0     19.4     0.000000    0.000000      0.0   \n",
              "\n",
              "                  val15_4       val16_4  val17_4  val18_4  val19_4  val20_4  \\\n",
              "telephone_number                                                              \n",
              "iWBn_13aJO           0.00 -1.490116e-09     0.00      0.0     39.0      4.0   \n",
              "8JWaD2wYoQ           0.00  0.000000e+00     0.00      0.0     81.0      1.0   \n",
              "sbPnQURi3j           0.65  2.500000e+00     0.05      0.6    239.0     62.0   \n",
              "qOy-JlqPTl           0.00  0.000000e+00     0.00      0.0    330.0    115.0   \n",
              "MNODhcb7h0           1.05  1.500000e-01     0.05      1.0    150.0    100.0   \n",
              "\n",
              "                  val21_4  val2_5  val4_5  val5_5  val6_5  val7_5  val8_5  \\\n",
              "telephone_number                                                            \n",
              "iWBn_13aJO            0.0     3.0    10.0     8.3     0.0     0.0     0.0   \n",
              "8JWaD2wYoQ            0.0     3.0     7.0     4.4     0.0     0.0     0.0   \n",
              "sbPnQURi3j            7.0     5.0    80.0   202.6     3.0     3.9     0.0   \n",
              "qOy-JlqPTl           97.0    25.0   100.0    97.5   114.0   103.0    42.0   \n",
              "MNODhcb7h0            2.0    10.0   114.0   227.3     1.0     1.5     1.0   \n",
              "\n",
              "                  val9_5  val10_5  val11_5       val12_5     val13_5  val14_5  \\\n",
              "telephone_number                                                                \n",
              "iWBn_13aJO           0.0      0.0      0.0      0.000000    0.000000      0.0   \n",
              "8JWaD2wYoQ           0.0      0.0      0.0    463.484166  198.153160      0.0   \n",
              "sbPnQURi3j           0.0      3.0      3.9      0.000000    0.000000      0.0   \n",
              "qOy-JlqPTl          35.4     72.0     67.6  10192.908145   65.786654      0.0   \n",
              "MNODhcb7h0           1.5      0.0      0.0      0.000000    0.000000      0.0   \n",
              "\n",
              "                  val15_5  val16_5  val17_5  val18_5  val19_5  val20_5  \\\n",
              "telephone_number                                                         \n",
              "iWBn_13aJO            0.0     0.91      0.0      0.0     57.0     10.0   \n",
              "8JWaD2wYoQ            0.0     0.64      0.0      0.0     88.0      7.0   \n",
              "sbPnQURi3j            0.5     2.35      0.0      0.5    219.0     80.0   \n",
              "qOy-JlqPTl            0.0     0.00      0.0      0.0    307.0    100.0   \n",
              "MNODhcb7h0            0.1     0.00      0.1      0.0    132.0    114.0   \n",
              "\n",
              "                  val21_5  val2_6  val4_6  val5_6  val6_6  val7_6  val8_6  \\\n",
              "telephone_number                                                            \n",
              "iWBn_13aJO            0.0     3.0     7.0     4.4     0.0     0.0     0.0   \n",
              "8JWaD2wYoQ            0.0     2.0     0.0     0.0     2.0     4.3     1.0   \n",
              "sbPnQURi3j            3.0     5.0    87.0   110.6     4.0     2.7     0.0   \n",
              "qOy-JlqPTl          114.0    25.0   142.0   121.3    80.0    60.6    29.0   \n",
              "MNODhcb7h0            1.0    10.0    96.0   163.1     0.0     0.0     0.0   \n",
              "\n",
              "                  val9_6  val10_6  val11_6      val12_6     val13_6  val14_6  \\\n",
              "telephone_number                                                               \n",
              "iWBn_13aJO           0.0      0.0      0.0     0.000000    0.000000      0.0   \n",
              "8JWaD2wYoQ           1.4      1.0      3.0   207.238037  206.536585      0.0   \n",
              "sbPnQURi3j           0.0      4.0      2.7     0.000000    0.000000      0.0   \n",
              "qOy-JlqPTl          21.0     51.0     39.6  9260.032768   51.885746      0.0   \n",
              "MNODhcb7h0           0.0      0.0      0.0     0.000000    0.000000      0.0   \n",
              "\n",
              "                  val15_6  val16_6  val17_6  val18_6  val19_6  val20_6  \\\n",
              "telephone_number                                                         \n",
              "iWBn_13aJO           0.00     0.56     0.00     0.00     60.0      7.0   \n",
              "8JWaD2wYoQ           0.40     0.00     0.16     0.24    156.0      0.0   \n",
              "sbPnQURi3j           0.25     2.35     0.00     0.25    241.0     87.0   \n",
              "qOy-JlqPTl           0.00     0.00     0.00     0.00    299.0    142.0   \n",
              "MNODhcb7h0           0.00     0.80     0.00     0.00    121.0     96.0   \n",
              "\n",
              "                  val21_6  target  \n",
              "telephone_number                   \n",
              "iWBn_13aJO            0.0       1  \n",
              "8JWaD2wYoQ            2.0       1  \n",
              "sbPnQURi3j            4.0       1  \n",
              "qOy-JlqPTl           80.0       1  \n",
              "MNODhcb7h0            0.0       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae58ec40-3843-491b-9822-6c747293688d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>trf</th>\n",
              "      <th>age</th>\n",
              "      <th>gndr</th>\n",
              "      <th>tenure</th>\n",
              "      <th>age_dev</th>\n",
              "      <th>dev_man</th>\n",
              "      <th>device_os_name</th>\n",
              "      <th>dev_num</th>\n",
              "      <th>is_dualsim</th>\n",
              "      <th>simcard_type</th>\n",
              "      <th>region</th>\n",
              "      <th>val2_1</th>\n",
              "      <th>val3_1</th>\n",
              "      <th>val4_1</th>\n",
              "      <th>val5_1</th>\n",
              "      <th>val6_1</th>\n",
              "      <th>val7_1</th>\n",
              "      <th>val8_1</th>\n",
              "      <th>val9_1</th>\n",
              "      <th>val10_1</th>\n",
              "      <th>val11_1</th>\n",
              "      <th>val12_1</th>\n",
              "      <th>val13_1</th>\n",
              "      <th>val14_1</th>\n",
              "      <th>val15_1</th>\n",
              "      <th>val16_1</th>\n",
              "      <th>val17_1</th>\n",
              "      <th>val18_1</th>\n",
              "      <th>val19_1</th>\n",
              "      <th>val20_1</th>\n",
              "      <th>val21_1</th>\n",
              "      <th>val2_2</th>\n",
              "      <th>val3_2</th>\n",
              "      <th>val4_2</th>\n",
              "      <th>val5_2</th>\n",
              "      <th>val6_2</th>\n",
              "      <th>val7_2</th>\n",
              "      <th>val8_2</th>\n",
              "      <th>val9_2</th>\n",
              "      <th>val10_2</th>\n",
              "      <th>val11_2</th>\n",
              "      <th>val12_2</th>\n",
              "      <th>val13_2</th>\n",
              "      <th>val14_2</th>\n",
              "      <th>val15_2</th>\n",
              "      <th>val16_2</th>\n",
              "      <th>val17_2</th>\n",
              "      <th>val18_2</th>\n",
              "      <th>val19_2</th>\n",
              "      <th>val20_2</th>\n",
              "      <th>val21_2</th>\n",
              "      <th>val2_3</th>\n",
              "      <th>val3_3</th>\n",
              "      <th>val4_3</th>\n",
              "      <th>val5_3</th>\n",
              "      <th>val6_3</th>\n",
              "      <th>val7_3</th>\n",
              "      <th>val8_3</th>\n",
              "      <th>val9_3</th>\n",
              "      <th>val10_3</th>\n",
              "      <th>val11_3</th>\n",
              "      <th>val12_3</th>\n",
              "      <th>val13_3</th>\n",
              "      <th>val14_3</th>\n",
              "      <th>val15_3</th>\n",
              "      <th>val16_3</th>\n",
              "      <th>val17_3</th>\n",
              "      <th>val18_3</th>\n",
              "      <th>val19_3</th>\n",
              "      <th>val20_3</th>\n",
              "      <th>val21_3</th>\n",
              "      <th>val2_4</th>\n",
              "      <th>val4_4</th>\n",
              "      <th>val5_4</th>\n",
              "      <th>val6_4</th>\n",
              "      <th>val7_4</th>\n",
              "      <th>val8_4</th>\n",
              "      <th>val9_4</th>\n",
              "      <th>val10_4</th>\n",
              "      <th>val11_4</th>\n",
              "      <th>val12_4</th>\n",
              "      <th>val13_4</th>\n",
              "      <th>val14_4</th>\n",
              "      <th>val15_4</th>\n",
              "      <th>val16_4</th>\n",
              "      <th>val17_4</th>\n",
              "      <th>val18_4</th>\n",
              "      <th>val19_4</th>\n",
              "      <th>val20_4</th>\n",
              "      <th>val21_4</th>\n",
              "      <th>val2_5</th>\n",
              "      <th>val4_5</th>\n",
              "      <th>val5_5</th>\n",
              "      <th>val6_5</th>\n",
              "      <th>val7_5</th>\n",
              "      <th>val8_5</th>\n",
              "      <th>val9_5</th>\n",
              "      <th>val10_5</th>\n",
              "      <th>val11_5</th>\n",
              "      <th>val12_5</th>\n",
              "      <th>val13_5</th>\n",
              "      <th>val14_5</th>\n",
              "      <th>val15_5</th>\n",
              "      <th>val16_5</th>\n",
              "      <th>val17_5</th>\n",
              "      <th>val18_5</th>\n",
              "      <th>val19_5</th>\n",
              "      <th>val20_5</th>\n",
              "      <th>val21_5</th>\n",
              "      <th>val2_6</th>\n",
              "      <th>val4_6</th>\n",
              "      <th>val5_6</th>\n",
              "      <th>val6_6</th>\n",
              "      <th>val7_6</th>\n",
              "      <th>val8_6</th>\n",
              "      <th>val9_6</th>\n",
              "      <th>val10_6</th>\n",
              "      <th>val11_6</th>\n",
              "      <th>val12_6</th>\n",
              "      <th>val13_6</th>\n",
              "      <th>val14_6</th>\n",
              "      <th>val15_6</th>\n",
              "      <th>val16_6</th>\n",
              "      <th>val17_6</th>\n",
              "      <th>val18_6</th>\n",
              "      <th>val19_6</th>\n",
              "      <th>val20_6</th>\n",
              "      <th>val21_6</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>telephone_number</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>iWBn_13aJO</th>\n",
              "      <td>J</td>\n",
              "      <td>50</td>\n",
              "      <td>M</td>\n",
              "      <td>6957.0</td>\n",
              "      <td>1263</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>Android OS</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>GAKH</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>14</td>\n",
              "      <td>7.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>79</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>1.05</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.00</td>\n",
              "      <td>64.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>10.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>68.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-1.490116e-09</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>60.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8JWaD2wYoQ</th>\n",
              "      <td>M</td>\n",
              "      <td>52</td>\n",
              "      <td>M</td>\n",
              "      <td>1041.0</td>\n",
              "      <td>125</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>Android OS</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4G</td>\n",
              "      <td>MASALLI</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6</td>\n",
              "      <td>2.9</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>176.447991</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>118</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1.06</td>\n",
              "      <td>1.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>21.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>117.043976</td>\n",
              "      <td>116.052454</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.08</td>\n",
              "      <td>3.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.08</td>\n",
              "      <td>118.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.71</td>\n",
              "      <td>3.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>34.4</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>160.147566</td>\n",
              "      <td>67.178972</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.40</td>\n",
              "      <td>1.28</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.16</td>\n",
              "      <td>125.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>182.922203</td>\n",
              "      <td>181.783154</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>463.484166</td>\n",
              "      <td>198.153160</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>207.238037</td>\n",
              "      <td>206.536585</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.24</td>\n",
              "      <td>156.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sbPnQURi3j</th>\n",
              "      <td>A</td>\n",
              "      <td>49</td>\n",
              "      <td>U</td>\n",
              "      <td>7176.0</td>\n",
              "      <td>345</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>Android OS</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4G</td>\n",
              "      <td>SAATLI</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>103</td>\n",
              "      <td>138.5</td>\n",
              "      <td>2</td>\n",
              "      <td>0.9</td>\n",
              "      <td>2</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36194.190184</td>\n",
              "      <td>36133.004917</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2.80</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>302</td>\n",
              "      <td>103</td>\n",
              "      <td>2</td>\n",
              "      <td>5.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>13857.365302</td>\n",
              "      <td>13827.253128</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.70</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>292.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>129.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.15</td>\n",
              "      <td>1.70</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.10</td>\n",
              "      <td>238.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.65</td>\n",
              "      <td>2.500000e+00</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.6</td>\n",
              "      <td>239.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>202.6</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.9</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.35</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>219.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>110.6</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.7</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>2.35</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>241.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>qOy-JlqPTl</th>\n",
              "      <td>M</td>\n",
              "      <td>49</td>\n",
              "      <td>M</td>\n",
              "      <td>4309.0</td>\n",
              "      <td>466</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>Android OS</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4G</td>\n",
              "      <td>BAKU</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>133</td>\n",
              "      <td>164.3</td>\n",
              "      <td>142</td>\n",
              "      <td>187.9</td>\n",
              "      <td>58</td>\n",
              "      <td>53.7</td>\n",
              "      <td>79</td>\n",
              "      <td>129.0</td>\n",
              "      <td>8181.310782</td>\n",
              "      <td>49.985725</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>411</td>\n",
              "      <td>133</td>\n",
              "      <td>142</td>\n",
              "      <td>25.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>209.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>49.1</td>\n",
              "      <td>126.0</td>\n",
              "      <td>124.3</td>\n",
              "      <td>7804.867967</td>\n",
              "      <td>65.904087</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>622.0</td>\n",
              "      <td>209.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>25.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>144.2</td>\n",
              "      <td>172.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>46.8</td>\n",
              "      <td>116.0</td>\n",
              "      <td>129.1</td>\n",
              "      <td>11567.962663</td>\n",
              "      <td>66.429559</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>542.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>130.7</td>\n",
              "      <td>97.0</td>\n",
              "      <td>124.5</td>\n",
              "      <td>29.0</td>\n",
              "      <td>34.6</td>\n",
              "      <td>68.0</td>\n",
              "      <td>89.8</td>\n",
              "      <td>8293.154886</td>\n",
              "      <td>44.728278</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>330.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>97.5</td>\n",
              "      <td>114.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>35.4</td>\n",
              "      <td>72.0</td>\n",
              "      <td>67.6</td>\n",
              "      <td>10192.908145</td>\n",
              "      <td>65.786654</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>307.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>121.3</td>\n",
              "      <td>80.0</td>\n",
              "      <td>60.6</td>\n",
              "      <td>29.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>39.6</td>\n",
              "      <td>9260.032768</td>\n",
              "      <td>51.885746</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>299.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MNODhcb7h0</th>\n",
              "      <td>A</td>\n",
              "      <td>50</td>\n",
              "      <td>U</td>\n",
              "      <td>6999.0</td>\n",
              "      <td>695</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>Android OS</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4G</td>\n",
              "      <td>ASTARA</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>102</td>\n",
              "      <td>85.1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1271.287395</td>\n",
              "      <td>1268.585261</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>169</td>\n",
              "      <td>102</td>\n",
              "      <td>1</td>\n",
              "      <td>10.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>97.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>106.041120</td>\n",
              "      <td>105.620465</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>185.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>12.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>104.6</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>160.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>111.7</td>\n",
              "      <td>2.0</td>\n",
              "      <td>20.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.05</td>\n",
              "      <td>1.500000e-01</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>227.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>163.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>121.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae58ec40-3843-491b-9822-6c747293688d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ae58ec40-3843-491b-9822-6c747293688d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ae58ec40-3843-491b-9822-6c747293688d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d575afe1-71df-4318-a692-26e67370ede2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d575afe1-71df-4318-a692-26e67370ede2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d575afe1-71df-4318-a692-26e67370ede2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample = df.sample(frac=0.2, random_state=42) # 20% sample (shuffled)\n",
        "df_sample.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "703C7vVcjlZJ",
        "outputId": "9745f2bd-7f40-4b04-c6da-9d58ecc9fc56"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2000 entries, UdM8XYVVt9 to 0SqjCeBMFt\n",
            "Columns: 129 entries, trf to target\n",
            "dtypes: float64(111), int64(8), object(10)\n",
            "memory usage: 2.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_sample.drop('target', axis = 1)\n",
        "y = df_sample['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "HoifYdukpoKU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify columns by types\n",
        "val_cols = [col for col in X_train.columns if col.startswith('val')]\n",
        "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.difference(val_cols)\n",
        "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()"
      ],
      "metadata": {
        "id": "-127fCI5rhLk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA was applied to the val columns to reduce their high dimensionality and remove redundancy, making the model more efficient and less prone to overfitting."
      ],
      "metadata": {
        "id": "1-zgYfcYz3g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=0.95)),\n",
        "])\n",
        "\n",
        "numeric_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()),\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', CatBoostEncoder(handle_unknown='ignore')),\n",
        "])\n",
        "\n",
        "# combine all\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('val', val_pipeline, val_cols),\n",
        "    ('num', numeric_pipeline, numeric_cols),\n",
        "    ('cat', categorical_pipeline, categorical_cols),\n",
        "])"
      ],
      "metadata": {
        "id": "2Pqu0ZHpqrQj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest pipeline (classification)\n",
        "rf_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', RandomForestClassifier(random_state=42)),\n",
        "])\n",
        "\n",
        "# XGBoost pipeline (classification)\n",
        "xgb_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
        "])\n"
      ],
      "metadata": {
        "id": "QW274wpIuHrf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation with accuracy\n",
        "rf_scores = cross_val_score(rf_pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
        "xgb_scores = cross_val_score(xgb_pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "print(f\"RF CV Accuracy: {np.mean(rf_scores):.4f} ± {np.std(rf_scores):.4f}\")\n",
        "print(f\"XGB CV Accuracy: {np.mean(xgb_scores):.4f} ± {np.std(xgb_scores):.4f}\")\n",
        "\n",
        "# Cross-validation with weighted F1\n",
        "rf_f1_scores = cross_val_score(rf_pipeline, X_train, y_train, cv=5, scoring='f1_weighted')\n",
        "xgb_f1_scores = cross_val_score(xgb_pipeline, X_train, y_train, cv=5, scoring='f1_weighted')\n",
        "\n",
        "print(f\"RF CV F1 (weighted): {np.mean(rf_f1_scores):.4f} ± {np.std(rf_f1_scores):.4f}\")\n",
        "print(f\"XGB CV F1 (weighted): {np.mean(xgb_f1_scores):.4f} ± {np.std(xgb_f1_scores):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbADcBuGvwzt",
        "outputId": "15e4d999-d86b-4e43-ee69-6d7217d5392f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF CV Accuracy: 0.5719 ± 0.0221\n",
            "XGB CV Accuracy: 0.5669 ± 0.0318\n",
            "RF CV F1 (weighted): 0.5688 ± 0.0239\n",
            "XGB CV F1 (weighted): 0.5647 ± 0.0331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypothesis testing (paired t-test)\n",
        "stat, p_value = ttest_rel(rf_scores, xgb_scores)\n",
        "print(f\"Paired t-test p-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    better_model = 'Random Forest' if np.mean(rf_scores) > np.mean(xgb_scores) else 'XGBoost'\n",
        "    print(f\"Statistically significant difference. Better model: {better_model}\")\n",
        "else:\n",
        "    print(\"No statistically significant difference between models.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC60jBh3wEpc",
        "outputId": "f6dac855-59ea-46db-8da5-d7834919beba"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paired t-test p-value: 0.8279\n",
            "No statistically significant difference between models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    # Suggest hyperparameters\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 5),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 5),\n",
        "        'use_label_encoder': False\n",
        "    }\n",
        "\n",
        "    # Build classifier\n",
        "    model = XGBClassifier(\n",
        "        random_state=42,\n",
        "        eval_metric='logloss',\n",
        "        **params\n",
        "    )\n",
        "\n",
        "    # Create pipeline\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', model),\n",
        "    ])\n",
        "\n",
        "    # Evaluate with cross-validation (3-fold)\n",
        "    scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring='accuracy')\n",
        "\n",
        "    # We want to maximize accuracy\n",
        "    return scores.mean()\n",
        "\n",
        "# Run Optuna\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Print best results\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"  Accuracy: {trial.value:.4f}\")\n",
        "print(\"  Params:\")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOZDQjiB0tTG",
        "outputId": "137504bd-3573-4b46-8142-6f868b77430e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-13 22:19:02,329] A new study created in memory with name: no-name-a322a0c2-c009-42d6-a369-f433a3e7c1cd\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:02] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:03] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:19:04,730] Trial 0 finished with value: 0.5824942086931674 and parameters: {'n_estimators': 390, 'max_depth': 7, 'learning_rate': 0.1419947860556081, 'subsample': 0.9653561261059125, 'colsample_bytree': 0.7320920614011016, 'gamma': 1.1744758765203218, 'reg_alpha': 2.1647345135820872, 'reg_lambda': 2.631427319337784}. Best is trial 0 with value: 0.5824942086931674.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:05] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:07] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:19:08,803] Trial 1 finished with value: 0.5831301398580105 and parameters: {'n_estimators': 170, 'max_depth': 3, 'learning_rate': 0.02512424241990668, 'subsample': 0.767963158491047, 'colsample_bytree': 0.8442471342891225, 'gamma': 3.2578599872059097, 'reg_alpha': 3.0338497371155553, 'reg_lambda': 2.8646488636984113}. Best is trial 1 with value: 0.5831301398580105.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:19:10,530] Trial 2 finished with value: 0.5674988815575278 and parameters: {'n_estimators': 398, 'max_depth': 8, 'learning_rate': 0.17422192330991063, 'subsample': 0.6059818581045422, 'colsample_bytree': 0.6828712886656758, 'gamma': 4.810519753230149, 'reg_alpha': 1.7184437221615378, 'reg_lambda': 0.4968534306807587}. Best is trial 1 with value: 0.5831301398580105.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:13] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:19:14,100] Trial 3 finished with value: 0.5762566960155341 and parameters: {'n_estimators': 457, 'max_depth': 3, 'learning_rate': 0.02249047250981375, 'subsample': 0.6240645491244873, 'colsample_bytree': 0.8867734068903317, 'gamma': 4.142813041098821, 'reg_alpha': 0.2244233236896498, 'reg_lambda': 1.2276707836611749}. Best is trial 1 with value: 0.5831301398580105.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:14] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:14] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:15] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:19:16,140] Trial 4 finished with value: 0.5399840255965221 and parameters: {'n_estimators': 257, 'max_depth': 9, 'learning_rate': 0.25210181698585815, 'subsample': 0.7781017073183446, 'colsample_bytree': 0.6349835999350473, 'gamma': 0.9792557334935931, 'reg_alpha': 2.9658352789265945, 'reg_lambda': 1.4181237137559917}. Best is trial 1 with value: 0.5831301398580105.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:16] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:16] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:17] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:19:17,617] Trial 5 finished with value: 0.5799973297917941 and parameters: {'n_estimators': 298, 'max_depth': 10, 'learning_rate': 0.13889494381669995, 'subsample': 0.992705211570798, 'colsample_bytree': 0.6174841034323083, 'gamma': 3.5500573098759953, 'reg_alpha': 3.0859860619973616, 'reg_lambda': 4.96340556932564}. Best is trial 1 with value: 0.5831301398580105.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:17] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:18] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:19:20,878] Trial 6 finished with value: 0.5668723195442844 and parameters: {'n_estimators': 304, 'max_depth': 9, 'learning_rate': 0.11290729340980013, 'subsample': 0.6316325873159759, 'colsample_bytree': 0.9377325061850431, 'gamma': 4.211614831847661, 'reg_alpha': 4.192468428458847, 'reg_lambda': 2.7643981223152654}. Best is trial 1 with value: 0.5831301398580105.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:21] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:19:23,058] Trial 7 finished with value: 0.5700086430423509 and parameters: {'n_estimators': 471, 'max_depth': 10, 'learning_rate': 0.14286535795398111, 'subsample': 0.8775581615277845, 'colsample_bytree': 0.6923308778278865, 'gamma': 3.3760896497330757, 'reg_alpha': 2.4375969930668333, 'reg_lambda': 4.190663518731925}. Best is trial 1 with value: 0.5831301398580105.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:23] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:23] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:24] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:19:24,269] Trial 8 finished with value: 0.5887481173861003 and parameters: {'n_estimators': 115, 'max_depth': 5, 'learning_rate': 0.08662477132959676, 'subsample': 0.7167015169291521, 'colsample_bytree': 0.6244638876707039, 'gamma': 4.0329400621218685, 'reg_alpha': 3.777364578554851, 'reg_lambda': 3.9000346473753}. Best is trial 8 with value: 0.5887481173861003.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:24] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:25] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:25] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:19:26,360] Trial 9 finished with value: 0.5687484921521644 and parameters: {'n_estimators': 447, 'max_depth': 8, 'learning_rate': 0.1567866582251811, 'subsample': 0.9082764794550952, 'colsample_bytree': 0.7055213161350838, 'gamma': 2.4069169415963305, 'reg_alpha': 3.9939354437485832, 'reg_lambda': 1.9123986731928437}. Best is trial 8 with value: 0.5887481173861003.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:26] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:27] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:27] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:19:28,394] Trial 10 finished with value: 0.5862547519165772 and parameters: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.050115845335242416, 'subsample': 0.7080323304402256, 'colsample_bytree': 0.7855032738107302, 'gamma': 2.1794311629376555, 'reg_alpha': 4.663663710080582, 'reg_lambda': 3.8141642984507573}. Best is trial 8 with value: 0.5887481173861003.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:28] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:29] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:29] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:19:30,376] Trial 11 finished with value: 0.5906242899939803 and parameters: {'n_estimators': 111, 'max_depth': 5, 'learning_rate': 0.0569148158516382, 'subsample': 0.6995303772194241, 'colsample_bytree': 0.7947162085448713, 'gamma': 2.1495829040338763, 'reg_alpha': 4.874843851836602, 'reg_lambda': 3.836528549844432}. Best is trial 11 with value: 0.5906242899939803.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:30] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:31] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:33] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:19:34,683] Trial 12 finished with value: 0.5856305321912337 and parameters: {'n_estimators': 115, 'max_depth': 5, 'learning_rate': 0.05462217934441617, 'subsample': 0.701040836890715, 'colsample_bytree': 0.974479117626527, 'gamma': 1.7348066658180685, 'reg_alpha': 4.939626697987843, 'reg_lambda': 3.7179176289972387}. Best is trial 11 with value: 0.5906242899939803.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:34] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:36] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:37] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:19:38,330] Trial 13 finished with value: 0.5818805292633739 and parameters: {'n_estimators': 178, 'max_depth': 5, 'learning_rate': 0.0763684985970669, 'subsample': 0.7152022492154851, 'colsample_bytree': 0.773437690321121, 'gamma': 0.49423633075525686, 'reg_alpha': 4.043792841450207, 'reg_lambda': 4.997386709139029}. Best is trial 11 with value: 0.5906242899939803.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:38] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:39] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:41] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:19:42,364] Trial 14 finished with value: 0.5975035895562067 and parameters: {'n_estimators': 183, 'max_depth': 6, 'learning_rate': 0.011836043445278416, 'subsample': 0.8350286928223695, 'colsample_bytree': 0.8460256447276271, 'gamma': 2.739629633849611, 'reg_alpha': 3.729788166892277, 'reg_lambda': 3.5386814924579557}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:42] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:43] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:19:47,722] Trial 15 finished with value: 0.5868848273616704 and parameters: {'n_estimators': 198, 'max_depth': 6, 'learning_rate': 0.014114014734811884, 'subsample': 0.8480413248506408, 'colsample_bytree': 0.8597424123618161, 'gamma': 2.736196200908917, 'reg_alpha': 4.933068397253042, 'reg_lambda': 3.3695453937218716}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:19:50,803] Trial 16 finished with value: 0.5906242899939803 and parameters: {'n_estimators': 234, 'max_depth': 4, 'learning_rate': 0.03391398557270724, 'subsample': 0.8235030009378913, 'colsample_bytree': 0.8177534700329268, 'gamma': 1.706155374042439, 'reg_alpha': 3.4551756977621055, 'reg_lambda': 4.363372735657669}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:51] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:52] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:54] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:19:55,250] Trial 17 finished with value: 0.5831195995624606 and parameters: {'n_estimators': 149, 'max_depth': 6, 'learning_rate': 0.012945662476911306, 'subsample': 0.7674390889091243, 'colsample_bytree': 0.9038609023119482, 'gamma': 2.887029509042046, 'reg_alpha': 1.1967294688639036, 'reg_lambda': 3.4171496372054917}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:19:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:00] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:03] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:20:06,390] Trial 18 finished with value: 0.5843785793086971 and parameters: {'n_estimators': 233, 'max_depth': 7, 'learning_rate': 0.010195077828177445, 'subsample': 0.6719592608291822, 'colsample_bytree': 0.7483290603561371, 'gamma': 0.02432699944302108, 'reg_alpha': 4.442630859776489, 'reg_lambda': 2.041900121434349}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:06] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:07] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:20:08,652] Trial 19 finished with value: 0.581873502399674 and parameters: {'n_estimators': 143, 'max_depth': 4, 'learning_rate': 0.03282707213753627, 'subsample': 0.9221346854670324, 'colsample_bytree': 0.8139186071977677, 'gamma': 1.9140391398955434, 'reg_alpha': 3.532222051076159, 'reg_lambda': 3.1574097530762333}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:13] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:20:15,120] Trial 20 finished with value: 0.581866475535974 and parameters: {'n_estimators': 288, 'max_depth': 6, 'learning_rate': 0.04064759027884824, 'subsample': 0.8100086089763131, 'colsample_bytree': 0.9350789090905697, 'gamma': 1.2927629794988413, 'reg_alpha': 4.464631106314739, 'reg_lambda': 4.542419831230013}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:15] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:16] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:17] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:20:18,526] Trial 21 finished with value: 0.5899965568367871 and parameters: {'n_estimators': 221, 'max_depth': 4, 'learning_rate': 0.01952040181400289, 'subsample': 0.8283351669665493, 'colsample_bytree': 0.8223947360345288, 'gamma': 1.7500006947762416, 'reg_alpha': 3.4440269567128796, 'reg_lambda': 4.297387958019127}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:18] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:20:20,391] Trial 22 finished with value: 0.5856235053275339 and parameters: {'n_estimators': 197, 'max_depth': 4, 'learning_rate': 0.07205101679181, 'subsample': 0.8564934019857904, 'colsample_bytree': 0.8556093042413023, 'gamma': 2.504186480382023, 'reg_alpha': 3.444118072488618, 'reg_lambda': 4.34906736781841}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:20] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:21] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:20:25,364] Trial 23 finished with value: 0.5868719447782205 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.034324960575596704, 'subsample': 0.753624881056803, 'colsample_bytree': 0.7860229802269928, 'gamma': 1.5239247595639696, 'reg_alpha': 2.7063067467798003, 'reg_lambda': 3.627042323470153}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:25] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:26] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:27] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:20:27,673] Trial 24 finished with value: 0.5862559230605271 and parameters: {'n_estimators': 146, 'max_depth': 3, 'learning_rate': 0.028220035281667024, 'subsample': 0.8036791985566645, 'colsample_bytree': 0.8238461583530597, 'gamma': 2.1817878239843025, 'reg_alpha': 3.7602740934785333, 'reg_lambda': 4.649830215365978}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:27] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:29] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:30] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:20:31,450] Trial 25 finished with value: 0.5856281899033338 and parameters: {'n_estimators': 336, 'max_depth': 6, 'learning_rate': 0.018374730008006247, 'subsample': 0.8879334339587021, 'colsample_bytree': 0.8919268016489216, 'gamma': 2.9465339908439994, 'reg_alpha': 4.234266533770182, 'reg_lambda': 3.966600103834901}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:31] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:33] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:34] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:20:36,047] Trial 26 finished with value: 0.575616080274891 and parameters: {'n_estimators': 207, 'max_depth': 5, 'learning_rate': 0.04402475967952009, 'subsample': 0.6658032991507589, 'colsample_bytree': 0.7617598486853125, 'gamma': 0.7645142316259017, 'reg_alpha': 2.112969433757174, 'reg_lambda': 3.2188711626447084}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:36] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:39] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:40] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:20:41,740] Trial 27 finished with value: 0.5793789657862006 and parameters: {'n_estimators': 170, 'max_depth': 7, 'learning_rate': 0.016672046141367344, 'subsample': 0.7418211360174407, 'colsample_bytree': 0.7221720816303046, 'gamma': 1.946626759163365, 'reg_alpha': 4.607549594117801, 'reg_lambda': 2.33776198159876}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:41] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:42] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:43] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:20:44,333] Trial 28 finished with value: 0.5912531942951236 and parameters: {'n_estimators': 268, 'max_depth': 4, 'learning_rate': 0.07060156086194892, 'subsample': 0.821979445672959, 'colsample_bytree': 0.8064176734760888, 'gamma': 1.4281528691911345, 'reg_alpha': 3.153142049463467, 'reg_lambda': 3.080981859572245}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:45] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:20:47,053] Trial 29 finished with value: 0.571870761922831 and parameters: {'n_estimators': 380, 'max_depth': 6, 'learning_rate': 0.10052830710870808, 'subsample': 0.921770586620107, 'colsample_bytree': 0.7457148605945184, 'gamma': 1.4483556789760856, 'reg_alpha': 1.8551615913852006, 'reg_lambda': 2.930489639526238}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:52] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:20:53,408] Trial 30 finished with value: 0.5756195937067408 and parameters: {'n_estimators': 343, 'max_depth': 7, 'learning_rate': 0.06359936339527741, 'subsample': 0.8470884400594354, 'colsample_bytree': 0.8610602645726523, 'gamma': 1.0267582940930207, 'reg_alpha': 2.6899076752793833, 'reg_lambda': 2.4138359811080554}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:53] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:54] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:54] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:20:55,525] Trial 31 finished with value: 0.5850051413219405 and parameters: {'n_estimators': 256, 'max_depth': 4, 'learning_rate': 0.06506185775965691, 'subsample': 0.7958273833552076, 'colsample_bytree': 0.7997436975350241, 'gamma': 2.295671628528579, 'reg_alpha': 3.2396923795345227, 'reg_lambda': 4.101425251323032}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:56] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:57] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:20:57,810] Trial 32 finished with value: 0.5906242899939803 and parameters: {'n_estimators': 277, 'max_depth': 3, 'learning_rate': 0.04000451911455926, 'subsample': 0.8241772922167968, 'colsample_bytree': 0.8234623031876467, 'gamma': 2.6767925655591247, 'reg_alpha': 2.3914516402432247, 'reg_lambda': 3.4766411715725827}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:58] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:20:59] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:01] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:21:05,082] Trial 33 finished with value: 0.5756207648506908 and parameters: {'n_estimators': 231, 'max_depth': 5, 'learning_rate': 0.02610360856682082, 'subsample': 0.9522941300640189, 'colsample_bytree': 0.8406491047702469, 'gamma': 0.5225095434839784, 'reg_alpha': 3.755389899643633, 'reg_lambda': 2.9389136423227487}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:05] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:05] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:06] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:21:06,619] Trial 34 finished with value: 0.5806156937973874 and parameters: {'n_estimators': 179, 'max_depth': 4, 'learning_rate': 0.20973396325057742, 'subsample': 0.7855661329976554, 'colsample_bytree': 0.8751160683634585, 'gamma': 3.033924370009891, 'reg_alpha': 2.7456804391330762, 'reg_lambda': 4.656056984887385}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:06] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:07] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:07] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:21:08,293] Trial 35 finished with value: 0.5743723254000042 and parameters: {'n_estimators': 131, 'max_depth': 3, 'learning_rate': 0.09911825023501358, 'subsample': 0.8661295984272126, 'colsample_bytree': 0.9138651189398981, 'gamma': 1.924011729444056, 'reg_alpha': 1.294591767192224, 'reg_lambda': 3.0741838216176567}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:13] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:21:16,606] Trial 36 finished with value: 0.5868731159221704 and parameters: {'n_estimators': 330, 'max_depth': 5, 'learning_rate': 0.010755753613696944, 'subsample': 0.8281219142956369, 'colsample_bytree': 0.801569532675605, 'gamma': 1.2918182821891246, 'reg_alpha': 3.228964901696579, 'reg_lambda': 3.5946722685088544}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:16] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:17] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:18] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:21:18,878] Trial 37 finished with value: 0.5768750600211274 and parameters: {'n_estimators': 271, 'max_depth': 4, 'learning_rate': 0.05089048958150315, 'subsample': 0.7449964493100143, 'colsample_bytree': 0.6683726015580481, 'gamma': 3.221028779149427, 'reg_alpha': 2.9469877022306163, 'reg_lambda': 0.5445368823276286}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:20] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:21:20,605] Trial 38 finished with value: 0.5868754582100705 and parameters: {'n_estimators': 240, 'max_depth': 3, 'learning_rate': 0.03267629986799487, 'subsample': 0.8922976750106547, 'colsample_bytree': 0.8349784364518258, 'gamma': 3.7122418291068056, 'reg_alpha': 3.684465833599386, 'reg_lambda': 2.710309057116467}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:20] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:23] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:25] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:21:29,454] Trial 39 finished with value: 0.5843715524449973 and parameters: {'n_estimators': 208, 'max_depth': 8, 'learning_rate': 0.021576349356331646, 'subsample': 0.7702357378026323, 'colsample_bytree': 0.768064415961834, 'gamma': 1.6511279421136134, 'reg_alpha': 0.28387059898776146, 'reg_lambda': 4.521991319312303}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:29] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:30] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:31] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:21:31,564] Trial 40 finished with value: 0.570001616178651 and parameters: {'n_estimators': 157, 'max_depth': 6, 'learning_rate': 0.12554762667638356, 'subsample': 0.6559952279318463, 'colsample_bytree': 0.7279514305759289, 'gamma': 2.5647314859432164, 'reg_alpha': 4.081809483720695, 'reg_lambda': 4.049534578895196}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:31] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:32] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:33] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:21:33,722] Trial 41 finished with value: 0.5862442116210272 and parameters: {'n_estimators': 278, 'max_depth': 3, 'learning_rate': 0.040070130653731724, 'subsample': 0.8280002572464894, 'colsample_bytree': 0.804297677841309, 'gamma': 2.6753942348035364, 'reg_alpha': 2.2718973402733176, 'reg_lambda': 3.4799460353518827}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:33] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:34] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:35] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:21:35,712] Trial 42 finished with value: 0.5875043625112137 and parameters: {'n_estimators': 268, 'max_depth': 3, 'learning_rate': 0.061315786947989594, 'subsample': 0.8210397941611144, 'colsample_bytree': 0.8366765706608605, 'gamma': 2.1505901056682006, 'reg_alpha': 2.994747120490222, 'reg_lambda': 3.7987754772521196}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:35] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:36] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:37] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:21:37,587] Trial 43 finished with value: 0.5824977221250173 and parameters: {'n_estimators': 299, 'max_depth': 4, 'learning_rate': 0.08327202667381485, 'subsample': 0.8398141505902733, 'colsample_bytree': 0.7915424541643654, 'gamma': 3.2718747602343283, 'reg_alpha': 2.4063021468585855, 'reg_lambda': 3.3350969954357708}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:37] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:38] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:39] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:21:39,413] Trial 44 finished with value: 0.5874996779354137 and parameters: {'n_estimators': 313, 'max_depth': 3, 'learning_rate': 0.0452211295964346, 'subsample': 0.8686588883640383, 'colsample_bytree': 0.8727468140621698, 'gamma': 4.4113043188868595, 'reg_alpha': 3.2457868130047487, 'reg_lambda': 2.509909682714882}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:39] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:42] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:21:45,439] Trial 45 finished with value: 0.5787465480532075 and parameters: {'n_estimators': 356, 'max_depth': 5, 'learning_rate': 0.029259905244317378, 'subsample': 0.7913337038123389, 'colsample_bytree': 0.8188602713142329, 'gamma': 2.408990691800002, 'reg_alpha': 2.0296549523322245, 'reg_lambda': 3.806166507301512}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:45] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:21:47,064] Trial 46 finished with value: 0.5612391171448446 and parameters: {'n_estimators': 103, 'max_depth': 5, 'learning_rate': 0.2907770003800911, 'subsample': 0.6090652935772379, 'colsample_bytree': 0.8501509064754171, 'gamma': 2.044770235241346, 'reg_alpha': 1.6388410700276217, 'reg_lambda': 4.208137425797526}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:21:48,771] Trial 47 finished with value: 0.5925051471776602 and parameters: {'n_estimators': 184, 'max_depth': 4, 'learning_rate': 0.05451756246109968, 'subsample': 0.817403352577577, 'colsample_bytree': 0.7474302982776986, 'gamma': 3.748558952050284, 'reg_alpha': 2.549767640590278, 'reg_lambda': 4.868782053208682}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:21:50,096] Trial 48 finished with value: 0.578131697479464 and parameters: {'n_estimators': 123, 'max_depth': 4, 'learning_rate': 0.05827702688998752, 'subsample': 0.731184665726925, 'colsample_bytree': 0.6617740515732758, 'gamma': 3.880151117080742, 'reg_alpha': 3.893060220672869, 'reg_lambda': 4.912616281037874}. Best is trial 14 with value: 0.5975035895562067.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:50] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:50] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:21:51] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-13 22:21:51,449] Trial 49 finished with value: 0.5924981203139603 and parameters: {'n_estimators': 187, 'max_depth': 5, 'learning_rate': 0.09675552345852162, 'subsample': 0.6858276530904134, 'colsample_bytree': 0.7736268490146148, 'gamma': 4.536192936619125, 'reg_alpha': 4.786263096534869, 'reg_lambda': 4.848078877001382}. Best is trial 14 with value: 0.5975035895562067.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial:\n",
            "  Accuracy: 0.5975\n",
            "  Params:\n",
            "    n_estimators: 183\n",
            "    max_depth: 6\n",
            "    learning_rate: 0.011836043445278416\n",
            "    subsample: 0.8350286928223695\n",
            "    colsample_bytree: 0.8460256447276271\n",
            "    gamma: 2.739629633849611\n",
            "    reg_alpha: 3.729788166892277\n",
            "    reg_lambda: 3.5386814924579557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " optuna.visualization.plot_optimization_history(study).show()\n",
        " optuna.visualization.plot_param_importances(study).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GxDer8n74Obc",
        "outputId": "9a219d1d-4c12-427b-c319-87b75e260ef9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"62f15d2b-a635-4b4e-a5b3-f0b3a7220958\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"62f15d2b-a635-4b4e-a5b3-f0b3a7220958\")) {                    Plotly.newPlot(                        \"62f15d2b-a635-4b4e-a5b3-f0b3a7220958\",                        [{\"mode\":\"markers\",\"name\":\"Objective Value\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.5824942086931674,0.5831301398580105,0.5674988815575278,0.5762566960155341,0.5399840255965221,0.5799973297917941,0.5668723195442844,0.5700086430423509,0.5887481173861003,0.5687484921521644,0.5862547519165772,0.5906242899939803,0.5856305321912337,0.5818805292633739,0.5975035895562067,0.5868848273616704,0.5906242899939803,0.5831195995624606,0.5843785793086971,0.581873502399674,0.581866475535974,0.5899965568367871,0.5856235053275339,0.5868719447782205,0.5862559230605271,0.5856281899033338,0.575616080274891,0.5793789657862006,0.5912531942951236,0.571870761922831,0.5756195937067408,0.5850051413219405,0.5906242899939803,0.5756207648506908,0.5806156937973874,0.5743723254000042,0.5868731159221704,0.5768750600211274,0.5868754582100705,0.5843715524449973,0.570001616178651,0.5862442116210272,0.5875043625112137,0.5824977221250173,0.5874996779354137,0.5787465480532075,0.5612391171448446,0.5925051471776602,0.578131697479464,0.5924981203139603],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Best Value\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.5824942086931674,0.5831301398580105,0.5831301398580105,0.5831301398580105,0.5831301398580105,0.5831301398580105,0.5831301398580105,0.5831301398580105,0.5887481173861003,0.5887481173861003,0.5887481173861003,0.5906242899939803,0.5906242899939803,0.5906242899939803,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067,0.5975035895562067],\"type\":\"scatter\"},{\"marker\":{\"color\":\"#cccccc\"},\"mode\":\"markers\",\"name\":\"Infeasible Trial\",\"showlegend\":false,\"x\":[],\"y\":[],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Optimization History Plot\"},\"xaxis\":{\"title\":{\"text\":\"Trial\"}},\"yaxis\":{\"title\":{\"text\":\"Objective Value\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('62f15d2b-a635-4b4e-a5b3-f0b3a7220958');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"30cdb92a-9dc4-48bb-b3e3-03b9df0c164d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"30cdb92a-9dc4-48bb-b3e3-03b9df0c164d\")) {                    Plotly.newPlot(                        \"30cdb92a-9dc4-48bb-b3e3-03b9df0c164d\",                        [{\"cliponaxis\":false,\"hovertemplate\":[\"reg_alpha (FloatDistribution): 0.008938650847719994\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"max_depth (IntDistribution): 0.03930198559101519\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"n_estimators (IntDistribution): 0.04292052215870175\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"reg_lambda (FloatDistribution): 0.057968774936772445\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"subsample (FloatDistribution): 0.05932538900530426\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"colsample_bytree (FloatDistribution): 0.07551878597771605\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"gamma (FloatDistribution): 0.1093259209241655\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"learning_rate (FloatDistribution): 0.6066999705586048\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"],\"name\":\"Objective Value\",\"orientation\":\"h\",\"text\":[\"\\u003c0.01\",\"0.04\",\"0.04\",\"0.06\",\"0.06\",\"0.08\",\"0.11\",\"0.61\"],\"textposition\":\"outside\",\"x\":[0.008938650847719994,0.03930198559101519,0.04292052215870175,0.057968774936772445,0.05932538900530426,0.07551878597771605,0.1093259209241655,0.6066999705586048],\"y\":[\"reg_alpha\",\"max_depth\",\"n_estimators\",\"reg_lambda\",\"subsample\",\"colsample_bytree\",\"gamma\",\"learning_rate\"],\"type\":\"bar\"}],                        {\"title\":{\"text\":\"Hyperparameter Importances\"},\"xaxis\":{\"title\":{\"text\":\"Hyperparameter Importance\"}},\"yaxis\":{\"title\":{\"text\":\"Hyperparameter\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('30cdb92a-9dc4-48bb-b3e3-03b9df0c164d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7hCQbo-5hra8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}